\chapter{Lecture Review}

    We first introduce building blocks of our model, consisting Support Vector Machine(SVM), Multi-layer Peceptron
(MLP), Random Forest, Restricted Boltzmann Machine (RBM), Conditional Restricted Boltzmann Machine, and Hyper Model. A brief overview of these models, we can divide these models in to two categories, supervised learning and unsupervised learning. Supervised learning is just like how teachers and dear supervisors teach us, giving us some examples, which are "labeled" by them, and we learn from the examples, however unsupervised learning is teachers give us a criteria to identify different objects, and we use the criteria to learn.



\section{A brief theoretical look into Machine Learning}
    A general issue Machine Learning models want to resolve is how well we can approximate the true generative function $f:\mathbb{X}\rightarrow\mathbb{Y} $, which generates observable data.
   
    For instance, as an economist, we are interested in forecasting future GDP, or other economic indices. Assume that $GDP_{t+1} =f(C_{t})$ is the truth, and we want to use our algorithm to choose $g(x)$ to approximate our target function $f(x)$, given the data $(\mathbb{x_{n}},\mathbb{y_{n}}) $ generated by the target function $f(x)$.% general model setup
%place the image

When we have the general setup of machine learning models, % what's model learned? how we get g(x)


Since we got the optimal $g(x)$ from our hypothesis set$\mathbb{H}$, how we assessment whether we have 'learned' from the data, or we just memorizing the labels in our data set.% how we evaluate the model

\clearpage
\subsection*{Model Complexity}
\subsubsection*{VC dimension}
Now we introduce the concept of VC dimension.
VC dimension (for Vapnik Chervonenkis dimension) (Vapnik and Chervonenkis (1968, 1971), Vapnik (1979)) measures the capacity of a hypothesis space. Capacity is a measure of complexity and measures the expressive power, richness or flexibility of a set of functions by assessing how wiggly its members can be.

\begin{defn}
The Vapnik-Chervonenkis dimension of a hypothesis set $\mathcal{}{H}$, denoted by $d_{vc}(\mathcal{H})$ or simply $d_{vc}$, is the largest value of $\mathcal{N}$ for which $\mathcal{m_{\mathbb{H}}}(\mathcal{f(N)})= 2^{N}$. for all $\mathcal{N}$ then $d_{vc}=\infty$

\end{defn}
%
For example, assume $\mathcal{H}$ is a hypothesis set which is linear separation model(use a line to separate data), we are given pairs of ${x_{n}, y_{n}} $, and $ x_{n} \in \mathcal{R^2}$. How many different binary combinations can we assign to the data set?

\begin{center}
{
\includegraphics[width=7cm]{OOO}
}
\end{center}

%----- end VC dimension
\subsection*{Penalty for Model complexity}
Now we fuse above concepts together explaining the core philosophy of our modeling.
%----what is E_out ? E_val?


\subsection*{Cross Validation\& Model Selection}

\section{Supervised Learning}

\subsection{Support Vector Machine}

Cortes and Vapnik, Support Vector Machine(1995) is a discriminative model. We can write our problem into a general form.
$$ y = g(\sum^{K}_{i=1}{x_{i}}) \quad \textrm{and} \quad K \in \mathbb{R}$$
For basic model, $y \in \{-1, +1\}$  and $\ x_{i} \in \mathbb{R^{K}}$.
The formula shows SVMs can be easily applied on the regression model which we are familiar with. To introduce SVM we first introduce Hard-Margin Support Vector Machine(HmSVM).
\clearpage

The model can be written informally as follow.

$$\large{\max_{\mathbb{W}}} \ margin(\mathbb{W})$$
$$ s.t\ margin(\mathbb{W})=\min_{n=1...N} Distance(\mathbb{x_{n}},\mathbb{W})$$
$$ and\ \forall \ n \ y_{i}(\mathbb{w}^Tx_{n})>0$$
The maximization problem is to find the 'fattest' tube to perfectly separate our data, assuming the data set is separable(so called hard margin).
More clearly to illustrate SVM is to use graph, we have a data set which is already labeled in +1, and -1, ploted below.\\

\begin{center}
{
\includegraphics[width=7cm]{OOO}
}
\end{center}
The solution of hard-margin SVM is the red-colored tube which is the "fattest" from all potential candidates which can classify all training data.

As you can see from (1), hard-margin SVM does not assume any underlying distribution of our data set, but only the assumption of separability of our data set. However this condition can be easily loose by putting an bias term $\zeta$ 

%\subsubsection{Prime Problem}
More formal form of hard margin SVM is to rewrite the problem into quadratic form.

\subsubsection{Dual Problem}

More formal form of hard margin SVM is to rewrite the problem into quadratic form.

\subsubsection*{Kernel Transformation}
\subsubsection*{Poly Nominal Kernel}
\subsubsection*{Radial Basis Function Kernel}
\subsubsection*{Support Vectors}
Sparse solution


\subsection{Multi-layer Perceptron}
\subsubsection*{L1 and L2 regularization}

\subsection{Random Forest}
\subsubsection{Decision Tree}
\subsubsection*{Bagging}
\subsubsection*{Model Selection}
